{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('./data/train.csv',index_col='Webpage_id')\n",
    "y_train=X_train['Tag']\n",
    "X_train.drop(columns=['Tag'], inplace=True)\n",
    "X_test=pd.read_csv('./data/test.xls',index_col='Webpage_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlp=X_train[\"Url\"]\n",
    "urlp = urlp.replace('((www\\.)|(http?://)|(.com/))',' ',regex=True)\n",
    "special_char_list = ['\\.', '\\;', '\\?', '\\}', '\\)', '\\{', '\\(','/','\\-']\n",
    "for special_char in special_char_list:\n",
    "    urlp = urlp.replace(special_char, ' ',regex=True)\n",
    "X_train['Url']=urlp\n",
    "\n",
    "\n",
    "urlpt=X_test[\"Url\"]\n",
    "urlpt = urlpt.replace('((www\\.)|(http?://)|(.com/)|(https?://))',' ',regex=True)\n",
    "special_char_list = ['\\.', '\\;', '\\?', '\\}', '\\)', '\\{', '\\(','/','\\-']\n",
    "for special_char in special_char_list:\n",
    "    urlpt = urlpt.replace(special_char, ' ',regex=True)\n",
    "X_test['Url']=urlpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=list(y_train)\n",
    "X_train=list(X_train.iloc[:,-1])\n",
    "X_test1=list(X_test.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html_data = pd.read_csv('C:/Users/Hardika/Downloads/InnoplexH/html_data.csv')\n",
    "#html_data.head()\n",
    "#from bs4 import BeautifulSoup\n",
    "#soup = BeautifulSoup(html_data['Html'][0], \"html5lib\")\n",
    "#print(soup.find(name='title'))\n",
    "#print(soup.find(name='title'))\n",
    "#print(soup.findAll('articleBody'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TreebankWordTokenizer.tokenize of <nltk.tokenize.treebank.TreebankWordTokenizer object at 0x0000020AD4C37438>>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vect = CountVectorizer()\n",
    "# use TreeankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "vect.set_params(tokenizer=tokenizer.tokenize)\n",
    "\n",
    "# remove English stop words\n",
    "vect.set_params(stop_words='english')\n",
    "\n",
    "# include 1-grams and 2-grams consider both grams\n",
    "vect.set_params(ngram_range=(1, 2))\n",
    "\n",
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect.set_params(max_df=0.5)\n",
    "\n",
    "# learn the 'vocabulary' of the training data\n",
    "vect.fit(X_train)\n",
    "\n",
    "# examine the fitted vocabulary\n",
    "#vect.get_feature_names()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vect.transform(X_train)\n",
    "test_vectors1 = vect.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hardika\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.classification import classification_report, accuracy_score\n",
    "from sklearn.cross_validation import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "clinicalTrials       0.90      0.99      0.94      2839\n",
      "   conferences       0.82      0.85      0.83      4666\n",
      "         forum       0.85      0.93      0.89      4503\n",
      "    guidelines       0.80      0.77      0.78      1329\n",
      "          news       0.64      0.70      0.67      7992\n",
      "        others       0.78      0.64      0.70     17417\n",
      "       profile       0.76      0.76      0.76      5196\n",
      "   publication       0.81      0.95      0.87      7705\n",
      "        thesis       0.91      1.00      0.95      1800\n",
      "\n",
      "   avg / total       0.78      0.78      0.78     53447\n",
      "\n",
      "ACCURACY:: 0.7821206054596141\n"
     ]
    }
   ],
   "source": [
    "preds = cross_val_predict(nb,train_vectors,y_train,cv=10)\n",
    "print(classification_report(y_train, preds))\n",
    "print(\"ACCURACY::\", accuracy_score(preds, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred1=nb.predict(test_vectors1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "S11 = X_test.index.values\n",
    "S22 = pd.DataFrame(y_pred1)\n",
    "\n",
    "FR= pd.DataFrame()\n",
    "FR['Webpage_id']=S11\n",
    "FR['Tag']=S22\n",
    "\n",
    "FR.to_csv('./Submission/Summary_SubmissionF1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "clinicalTrials       0.99      0.99      0.99      2839\n",
      "   conferences       0.91      0.84      0.88      4666\n",
      "         forum       0.98      0.74      0.84      4503\n",
      "    guidelines       0.87      0.82      0.84      1329\n",
      "          news       0.80      0.67      0.73      7992\n",
      "        others       0.71      0.89      0.79     17417\n",
      "       profile       0.88      0.63      0.74      5196\n",
      "   publication       0.91      0.90      0.90      7705\n",
      "        thesis       0.99      1.00      0.99      1800\n",
      "\n",
      "   avg / total       0.84      0.82      0.82     53447\n",
      "\n",
      "ACCURACY:: 0.8243867756843228\n"
     ]
    }
   ],
   "source": [
    "svcl = SGDClassifier(loss='hinge',penalty='l2',alpha=1e-3,n_iter=5,random_state=9)\n",
    "svcl.fit(train_vectors, y_train)\n",
    "predsvcl = cross_val_predict(svcl,train_vectors,y_train,cv=10)\n",
    "print(classification_report(y_train, predsvcl))\n",
    "print(\"ACCURACY::\", accuracy_score(predsvcl, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2=svcl.predict(test_vectors1)\n",
    "svcI = X_test.index.values\n",
    "svcT = pd.DataFrame(y_pred2)\n",
    "\n",
    "FRSvc= pd.DataFrame()\n",
    "FRSvc['Webpage_id']=svcI\n",
    "FRSvc['Tag']=svcT\n",
    "\n",
    "FRSvc.to_csv('./Submission/Summary_SubmissionFSgd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "clinicalTrials       0.99      0.99      0.99      2839\n",
      "   conferences       0.90      0.89      0.90      4666\n",
      "         forum       0.98      0.77      0.86      4503\n",
      "    guidelines       0.84      0.85      0.84      1329\n",
      "          news       0.78      0.69      0.73      7992\n",
      "        others       0.74      0.86      0.80     17417\n",
      "       profile       0.87      0.68      0.77      5196\n",
      "   publication       0.90      0.92      0.91      7705\n",
      "        thesis       1.00      1.00      1.00      1800\n",
      "\n",
      "   avg / total       0.84      0.83      0.83     53447\n",
      "\n",
      "ACCURACY:: 0.8339289389488652\n"
     ]
    }
   ],
   "source": [
    "OvR = OneVsRestClassifier(LogisticRegression(random_state=9))\n",
    "OvR.fit(train_vectors, y_train)\n",
    "predsOvR = cross_val_predict(OvR,train_vectors,y_train,cv=10)\n",
    "print(classification_report(y_train, predsOvR))\n",
    "print(\"ACCURACY::\", accuracy_score(predsOvR, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3=OvR.predict(test_vectors1)\n",
    "OvrI = X_test.index.values\n",
    "OvrT = pd.DataFrame(y_pred3)\n",
    "\n",
    "FROvr= pd.DataFrame()\n",
    "FROvr['Webpage_id']=OvrI\n",
    "FROvr['Tag']=OvrT\n",
    "\n",
    "FROvr.to_csv('./Submission/Summary_SubmissionFOvr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finalized OneVsRestClassifier Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
